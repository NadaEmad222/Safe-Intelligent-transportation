{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## downloading necessary packages and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics\n",
    "%pip install pickle\n",
    "%nvidia-smi    #for working on GPU\n",
    "%pip install torch\n",
    "%pip install tensorflow\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning YOLOV9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the vehicle orientation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nada\\yoloproject\\accident prediction\\Github project\n",
      "Download and extraction successful! Files are extracted to c:\\Users\\Nada\\yoloproject\\accident prediction\\Github project\\dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "# creating a directory for the dataset\n",
    "extraction_path = os.path.join(HOME, \"Object_Detection_Dataset\")\n",
    "os.makedirs(extraction_path)\n",
    "\n",
    "# download the dataset zip folder\n",
    "url = 'https://sekilab-students.s3.ap-northeast-1.amazonaws.com/2021/vehicle-orientation-dataset/vehicle-orientation-5.zip'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Open the content of the response as a byte stream\n",
    "    zip_content = io.BytesIO(response.content)\n",
    "    \n",
    "    # Use the zipfile library to extract the content\n",
    "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
    "        # Extract all the contents into the specified directory\n",
    "        zip_ref.extractall(extraction_path)\n",
    "    print(f\"Download and extraction successful! Files are extracted to {extraction_path}\")\n",
    "else:\n",
    "    print(\"Failed to download the file. Status code:\", response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Please note: The directories for the training, testing, and validation image folders specified in the 'data.yaml' file must be MANUALLY UPDATED before running the following cells to reflect the full path where the files are located.\")\n",
    "print(\"For the training images, set the path as follows: \" + f'{HOME}\\\\Object_Detection_Dataset\\\\vehicle-orientation-5\\\\train\\\\images')\n",
    "print(\"For the validation images, set the path as follows: \" + f'{HOME}\\\\Object_Detection_Dataset\\\\vehicle-orientation-5\\\\validate\\\\images')\n",
    "print(\"For the testing images, set the path as follows: \" + f'{HOME}\\\\Object_Detection_Dataset\\\\vehicle-orientation-5\\\\test\\\\images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pretrained yolov9 model and further train it on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nada\\yoloproject\\accident prediction\\Github project\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov9e.pt')  # load an official model\n",
    "dataset = f'{HOME}\\\\Object_Detection_Dataset\\\\vehicle-orientation-5\\\\data.yaml'\n",
    "results = model.train(data=dataset ,device=0, epochs=10,batch=8 ) # further train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the yolov9 performance on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = YOLO('/runs/detect/train/weights/best.pt')\n",
    "metrics = model.val(data=dataset)  \n",
    "print(metrics)\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the CCD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nada\\yoloproject\\accident prediction\\Github project\\CCD_Normal_Sequences\\train\n",
      "c:\\Users\\Nada\\yoloproject\\accident prediction\\Github project\\CCD_Normal_Sequences\\train\\Accident_Prediction_Dataset\\accidents\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "# creating a directory for the dataset\n",
    "extraction_path = os.path.join(HOME, \"Accident_Prediction_Dataset\\\\accidents\")\n",
    "os.makedirs(extraction_path)\n",
    "\n",
    "# download the accident videos dataset zip folder\n",
    "url = 'https://drive.google.com/uc?export=download&id=1fmcwGhr8JT9YfLUrlcvuCZ3ychi2eyFP'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Open the content of the response as a byte stream\n",
    "    zip_content = io.BytesIO(response.content)\n",
    "    \n",
    "    # Use the zipfile library to extract the content\n",
    "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
    "        # Extract all the contents into the specified directory\n",
    "        zip_ref.extractall(extraction_path)\n",
    "    print(f\"The Accident videos are downloaded and extracted successfully in {extraction_path}\")\n",
    "else:\n",
    "    print(\"Failed to download accident videos. Status code:\", response.status_code)\n",
    "\n",
    "\n",
    "\n",
    "extraction_path = os.path.join(HOME, \"Accident_Prediction_Dataset\\\\Non_accidents\")\n",
    "os.makedirs(extraction_path)\n",
    "\n",
    "# download the non-accident videos dataset zip folder\n",
    "url = 'https://drive.google.com/uc?export=download&id=11ErpWQmmV5au2JOQVxwYl3ebtuugdlan'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Open the content of the response as a byte stream\n",
    "    zip_content = io.BytesIO(response.content)\n",
    "    \n",
    "    # Use the zipfile library to extract the content\n",
    "    with zipfile.ZipFile(zip_content, 'r') as zip_ref:\n",
    "        # Extract all the contents into the specified directory\n",
    "        zip_ref.extractall(extraction_path)\n",
    "    print(f\"The Non-accident videos are downloaded and extracted successfully in {extraction_path}\")\n",
    "else:\n",
    "    print(\"Failed to download non-accident videos. Status code:\", response.status_code)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process accident videos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#make sure that we are in the main project directory\n",
    "print(os.getcwd())   \n",
    "\n",
    "dataset_folder = os.path.join(HOME,'Accident_Prediction_Dataset\\\\accidents')\n",
    "\n",
    "max_frames=50\n",
    "max_objects=30\n",
    "no_of_features=8\n",
    "\n",
    "model_path = os.path.join(HOME,'runs\\\\detect\\\\train\\\\weights\\\\best.pt')\n",
    "# loop over the videos in the dataset folder \n",
    "for filename in os.listdir(dataset_folder):\n",
    "    # load the yolo model\n",
    "    model = YOLO(model_path) \n",
    "\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(dataset_folder, filename)\n",
    "        print('opening a new video')\n",
    "        # Initialize a list to hold the sequences of frames\n",
    "        sequences = []       \n",
    "        max_i = 0\n",
    "        # Store the track history\n",
    "        track_history = defaultdict(lambda: [])\n",
    "    \n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # keep track of track_ids\n",
    "        prev_track_ids = None\n",
    "        # Loop through the video frames\n",
    "        while cap.isOpened():\n",
    "            # Read a frame from the video\n",
    "            success, frame = cap.read()\n",
    "    \n",
    "            if success:\n",
    "                # Initialize a list to hold the detections of the current frame \n",
    "                current_sequence = [None]* max_objects\n",
    "    \n",
    "                # Run YOLOv9 tracking on the frame, persisting tracks between frames\n",
    "                results = model.track(frame, persist=True)\n",
    "\n",
    "                # extract info/features from the results of yolov9 detection \n",
    "                if results is not None and results[0].boxes is not None :\n",
    "\n",
    "                    #if the tracker failed to identify objects using ID, assume the track_ids of the previous frame \n",
    "                    if(results[0].boxes.id is not None):\n",
    "                        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "                        prev_track_ids = track_ids\n",
    "                    elif(prev_track_ids is None):   # no track_ids from previous frames \n",
    "                        track_ids = [0]*len(results[0].boxes)\n",
    "                    else:\n",
    "                        track_ids = prev_track_ids\n",
    "                    boxes = results[0].boxes.xywh.cpu().tolist()\n",
    "                    confidences = results[0].boxes.conf.tolist()\n",
    "                    classes = results[0].boxes.cls.tolist()\n",
    "                    for box, track_id, conf, classID in zip(boxes, track_ids, confidences, classes):\n",
    "                        # extract the type and the orientation from the class_id\n",
    "                        if(int(classID)==1):\n",
    "                            class_id = 1\n",
    "                            orientation = 1\n",
    "                        elif(int(classID)==2):\n",
    "                            class_id = 1\n",
    "                            orientation = 2\n",
    "                    \n",
    "                        elif(int(classID)==3):\n",
    "                            class_id = 1\n",
    "                            orientation = 3\n",
    "                            \n",
    "                        elif(int(classID)==4):\n",
    "                            class_id = 2\n",
    "                            orientation = 1\n",
    "                            \n",
    "                        elif(int(classID)==5):\n",
    "                            class_id = 2\n",
    "                            orientation = 2\n",
    "                            \n",
    "                        elif(int(classID)==6):\n",
    "                            class_id = 2\n",
    "                            orientation = 3\n",
    "                            \n",
    "                        elif(int(classID)==7):\n",
    "                            class_id = 3\n",
    "                            orientation = 1\n",
    "                            \n",
    "                        elif(int(classID)==8):\n",
    "                            class_id = 3\n",
    "                            orientation = 2\n",
    "                            \n",
    "                        elif(int(classID)==9):\n",
    "                            class_id = 3\n",
    "                            orientation = 3\n",
    "                            \n",
    "                        elif(int(classID)==10):\n",
    "                            class_id = 4\n",
    "                            orientation = 1\n",
    "                            \n",
    "                        elif(int(classID)==11):\n",
    "                            class_id = 4\n",
    "                            orientation = 2\n",
    "                            \n",
    "                        elif(int(classID)==12):\n",
    "                            class_id = 4\n",
    "                            orientation = 3\n",
    "                            \n",
    "                        elif(int(classID)==13):\n",
    "                            class_id = 5\n",
    "                            orientation = 1\n",
    "                            \n",
    "                        elif(int(classID)==14):\n",
    "                            class_id = 5\n",
    "                            orientation = 2\n",
    "                            \n",
    "                        else:#int(classID)==15\n",
    "                            class_id = 5\n",
    "                            orientation = 3\n",
    "                           \n",
    "                        x, y, w, h = box   \n",
    "                        conf = float (int (conf * 1000) / 1000)\n",
    "\n",
    "                        # calculate the distance moved by every object between 2 consecutive frames \n",
    "                        track = track_history[track_id]\n",
    "                        distance_moved = 0\n",
    "                        if(len(track)!=0):\n",
    "                            prev_position = track[-1]\n",
    "                            distance_moved = math.sqrt( math.pow(float(x)-prev_position[0] , 2) + math.pow(float(y)-prev_position[1] , 2) )\n",
    "\n",
    "                        track.append((float(x), float(y)))  # x, y center point  \n",
    "                        \n",
    "                        # add the object info to the list , keep consistency in sequences by TRACK_id\n",
    "                        if(track_id<30):\n",
    "                            current_sequence[track_id] = [ x, y, w, h, distance_moved, int(class_id) , int(orientation), conf]\n",
    "\n",
    "\n",
    "                # replace the None values with list of zeros for padding \n",
    "                for i in range(len(current_sequence)):\n",
    "                    if(current_sequence[i] is None):\n",
    "                        current_sequence[i] = [0]*no_of_features  \n",
    "\n",
    "                sequences.append(current_sequence)  \n",
    "         \n",
    "            else:\n",
    "                break\n",
    "    \n",
    "        # Release the video capture object and close the display window\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "         # Pad the video to have a maximum of 50 frames\n",
    "        while len(sequences) < max_frames:\n",
    "            sequences.append([[0]*no_of_features]*max_objects)\n",
    "        if len(sequences) > max_frames:\n",
    "            sequences = sequences[:max_frames]\n",
    "\n",
    "        sequences = np.array(sequences)\n",
    "        np.save(os.path.join(HOME,'CCD_Accident_Sequences\\\\train\\\\')+Path(filename).stem+\".npy\", sequences)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Non_accident videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#make sure that we are in the main project directory\n",
    "print(os.getcwd())   \n",
    "\n",
    "dataset_folder = os.path.join(HOME,'Accident_Prediction_Dataset\\\\Non_accidents')\n",
    "\n",
    "max_frames=50\n",
    "max_objects=30\n",
    "no_of_features=8\n",
    "\n",
    "model_path = os.path.join(HOME,'runs\\\\detect\\\\train\\\\weights\\\\best.pt')\n",
    "# loop over the videos in the dataset folder \n",
    "for filename in os.listdir(dataset_folder):\n",
    "    # load the yolo model\n",
    "    model = YOLO(model_path) \n",
    "\n",
    "    if filename.endswith('.mp4'):\n",
    "        video_path = os.path.join(dataset_folder, filename)\n",
    "        print('opening a new video')\n",
    "        # Initialize a list to hold the sequences of frames\n",
    "        sequences = []       \n",
    "\n",
    "        # Store the track history\n",
    "        track_history = defaultdict(lambda: [])\n",
    "    \n",
    "        # Open the video file\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # keep track of track_ids\n",
    "        prev_track_ids = None\n",
    "        # Loop through the video frames\n",
    "        while cap.isOpened():\n",
    "            # Read a frame from the video\n",
    "            success, frame = cap.read()\n",
    "    \n",
    "            if success:\n",
    "                # Initialize a list to hold the detections of the current frame \n",
    "                current_sequence = [None]* max_objects\n",
    "    \n",
    "                # Run YOLOv9 tracking on the frame, persisting tracks between frames\n",
    "                results = model.track(frame, persist=True)\n",
    "\n",
    "                # extract info/features from the results of yolov9 detection \n",
    "                if results is not None and results[0].boxes is not None :\n",
    "\n",
    "                    #if the tracker failed to identify objects using ID, assume the track_ids of the previous frame \n",
    "                    if(results[0].boxes.id is not None):\n",
    "                        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "                        prev_track_ids = track_ids\n",
    "                    elif(prev_track_ids is None):   # no track_ids from previous frames \n",
    "                        track_ids = [0]*len(results[0].boxes)\n",
    "                    else:\n",
    "                        track_ids = prev_track_ids\n",
    "                    boxes = results[0].boxes.xywh.cpu().tolist()\n",
    "                    confidences = results[0].boxes.conf.tolist()\n",
    "                    classes = results[0].boxes.cls.tolist()\n",
    "                    for box, track_id, conf, classID in zip(boxes, track_ids, confidences, classes):\n",
    "                        # extract the type and the orientation from the class_id\n",
    "                        if(int(classID)==1):\n",
    "                            class_id = 1\n",
    "                            orientation = 1\n",
    "                        elif(int(classID)==2):\n",
    "                            class_id = 1\n",
    "                            orientation = 2\n",
    "                    \n",
    "                        elif(int(classID)==3):\n",
    "                            class_id = 1\n",
    "                            orientation = 3\n",
    "                            \n",
    "                        elif(int(classID)==4):\n",
    "                            class_id = 2\n",
    "                            orientation = 1\n",
    "                            \n",
    "                        elif(int(classID)==5):\n",
    "                            class_id = 2\n",
    "                            orientation = 2\n",
    "                            \n",
    "                        elif(int(classID)==6):\n",
    "                            class_id = 2\n",
    "                            orientation = 3\n",
    "                            \n",
    "                        elif(int(classID)==7):\n",
    "                            class_id = 3\n",
    "                            orientation = 1\n",
    "                            \n",
    "                        elif(int(classID)==8):\n",
    "                            class_id = 3\n",
    "                            orientation = 2\n",
    "                            \n",
    "                        elif(int(classID)==9):\n",
    "                            class_id = 3\n",
    "                            orientation = 3\n",
    "                            \n",
    "                        elif(int(classID)==10):\n",
    "                            class_id = 4\n",
    "                            orientation = 1\n",
    "                            \n",
    "                        elif(int(classID)==11):\n",
    "                            class_id = 4\n",
    "                            orientation = 2\n",
    "                            \n",
    "                        elif(int(classID)==12):\n",
    "                            class_id = 4\n",
    "                            orientation = 3\n",
    "                            \n",
    "                        elif(int(classID)==13):\n",
    "                            class_id = 5\n",
    "                            orientation = 1\n",
    "                            \n",
    "                        elif(int(classID)==14):\n",
    "                            class_id = 5\n",
    "                            orientation = 2\n",
    "                            \n",
    "                        else:#int(classID)==15\n",
    "                            class_id = 5\n",
    "                            orientation = 3\n",
    "                           \n",
    "                        x, y, w, h = box   \n",
    "                        conf = float (int (conf * 1000) / 1000)\n",
    "\n",
    "                        # calculate the distance moved by every object between 2 consecutive frames \n",
    "                        track = track_history[track_id]\n",
    "                        distance_moved = 0\n",
    "                        if(len(track)!=0):\n",
    "                            prev_position = track[-1]\n",
    "                            distance_moved = math.sqrt( math.pow(float(x)-prev_position[0] , 2) + math.pow(float(y)-prev_position[1] , 2) )\n",
    "\n",
    "                        track.append((float(x), float(y)))  # x, y center point  \n",
    "                        \n",
    "                        # add the object info to the list , keep consistency in sequences by TRACK_id\n",
    "                        if(track_id<max_objects):\n",
    "                            current_sequence[track_id] = [ x, y, w, h, distance_moved, int(class_id) , int(orientation), conf]\n",
    "\n",
    "\n",
    "                # replace the None values with list of zeros for padding \n",
    "                for i in range(len(current_sequence)):\n",
    "                    if(current_sequence[i] is None):\n",
    "                        current_sequence[i] = [0]*no_of_features  \n",
    "\n",
    "                sequences.append(current_sequence)  \n",
    "         \n",
    "            else:\n",
    "                break\n",
    "    \n",
    "        # Release the video capture object and close the display window\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "         # Pad the video to have a maximum of 50 frames\n",
    "        while len(sequences) < max_frames:\n",
    "            sequences.append([[0]*no_of_features]*max_objects)\n",
    "        if len(sequences) > max_frames:\n",
    "            sequences = sequences[:max_frames]\n",
    "\n",
    "        sequences = np.array(sequences)\n",
    "        np.save(os.path.join(HOME,'CCD_Normal_Sequences\\\\train\\\\')+Path(filename).stem+\".npy\", sequences)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the processed sequences into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nada\\yoloproject\\accident prediction\\Github project\\CCD_Normal_Sequences\\train\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# split the non_accident videos into 2 sets( train and validate )\n",
    "source_dir = f'{HOME}\\\\CCD_Normal_Sequences\\\\train'\n",
    "target_dir = f'{HOME}\\\\CCD_Normal_Sequences\\\\validate'\n",
    "\n",
    "os.makedirs(target_dir)\n",
    "\n",
    "files = os.listdir(source_dir)\n",
    "# for non accident videos: training set = 2400 and validation set = 600\n",
    "for file_name in files[:600]:\n",
    "    shutil.move(os.path.join(source_dir, file_name), target_dir)\n",
    "\n",
    "\n",
    "# split the accident videos into 2 sets( train and validate )\n",
    "source_dir = f'{HOME}\\\\CCD_Accident_Sequences\\\\train'\n",
    "target_dir = f'{HOME}\\\\CCD_Accident_Sequences\\\\validate'\n",
    "\n",
    "os.makedirs(target_dir)\n",
    "\n",
    "files = os.listdir(source_dir)\n",
    "# for accident videos: training set = 800 and validation set = 200\n",
    "for file_name in files[:200]:\n",
    "    shutil.move(os.path.join(source_dir, file_name), target_dir)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build and train the LSTM model on the processed sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "from tensorflow.keras.layers import LSTM, Dense , Dropout,Masking\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ===================== # customized functions for the evaluation matrices\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# a function for loading the sequences \n",
    "def load_data(data_dir1 , data_dir2):\n",
    "    x_train =[]\n",
    "    y_train =[]\n",
    "    # Loop through text files in data_dir\n",
    "    for filename in os.listdir(data_dir1):\n",
    "    # Read features from text file\n",
    "        data = np.load(data_dir1+\"/\"+filename)\n",
    "        x_train.append(data)\n",
    "        y_train.append(1.0)\n",
    "        \n",
    "    for filename in os.listdir(data_dir2):\n",
    "    # Read features from text file\n",
    "        data = np.load(data_dir2+\"/\"+filename)\n",
    "        x_train.append(data)\n",
    "        y_train.append(0.0)  \n",
    "        \n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "# ===================== # the LSTM model\n",
    "\n",
    "# size parameters\n",
    "max_frames=50\n",
    "max_objects=30\n",
    "no_of_features=8\n",
    "\n",
    "# Define model parameters\n",
    "num_lstm_units = 128\n",
    "num_dense_units = 64\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# load the data for training\n",
    "x_train , y_train =  load_data(f'{HOME}\\\\CCD_Accident_Sequences\\\\train',f'{HOME}\\\\CCD_Normal_Sequences\\\\train')     # read sequences from DESK\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_train = x_train.reshape(3200, -1, max_objects*no_of_features) # flatten each frame to 1D\n",
    "\n",
    "# validation data\n",
    "x_val ,y_val =  load_data( f'{HOME}\\\\CCD_Accident_Sequences\\\\validate',f'{HOME}\\\\CCD_Normal_Sequences\\\\validate'  )     # read sequences from DESK\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "# write the number of validation examples available\n",
    "x_val = x_val.reshape(800, -1, max_objects*no_of_features) # flatten each frame to 1D\n",
    "\n",
    "# Define the model\n",
    "model1 = tf.keras.Sequential()\n",
    "\n",
    "model1.add(Masking( mask_value=0 , input_shape=( max_frames, max_objects*no_of_features)))\n",
    "\n",
    "model1.add(LSTM(num_lstm_units))\n",
    "model1.add(Dropout(dropout_rate))\n",
    "\n",
    "# Dense layer for prediction\n",
    "model1.add(Dense(num_dense_units, activation='relu' ,kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01) ) )\n",
    "model1.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output layer with sigmoid activation for probability\n",
    "model1.add(Dense(1, activation='sigmoid' , kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01) ))\n",
    "    \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "\n",
    "history = model1.fit(x_train, y_train, epochs=16, batch_size=16, validation_data=(x_val, y_val), callbacks = early_stopping )\n",
    "\n",
    "# Save the model after training\n",
    "model1.save('accident_prediction_LSTM_model.keras') \n",
    "\n",
    "with open('historyLSTM.pickle', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the history\n",
    "with open('historyLSTM.pickle', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0.4, 0.9)\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Access the values of custom metrics\n",
    "recall_values = history['recall_m']\n",
    "precision_values = history['precision_m']\n",
    "f1_values = history['f1_m']\n",
    "\n",
    "# Plotting\n",
    "\n",
    "plt.plot(recall_values, label='Recall')\n",
    "plt.title('Recall Over Epochs')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(10,50)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(precision_values, label='Precision')\n",
    "plt.title('Precision over Epochs')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(10,50)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(f1_values, label='F1 Score')\n",
    "plt.title('F1 Score over Epochs')\n",
    "plt.ylabel('F1_score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(10, 50)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The GRU model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build and train the GRU model on the processed sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "from tensorflow.keras.layers import GRU, Dense , Dropout ,Masking\n",
    "import pickle\n",
    "\n",
    "# ===================== # customized functions for the evaluation matrices\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# a function for loading the sequences \n",
    "def load_data(data_dir1 , data_dir2):\n",
    "    x_train =[]\n",
    "    y_train =[]\n",
    "    # Loop through text files in data_dir\n",
    "    for filename in os.listdir(data_dir1):\n",
    "    # Read features from text file\n",
    "        data = np.load(data_dir1+\"/\"+filename)\n",
    "        x_train.append(data)\n",
    "        y_train.append(1.0)\n",
    "        \n",
    "    for filename in os.listdir(data_dir2):\n",
    "    # Read features from text file\n",
    "        data = np.load(data_dir2+\"/\"+filename)\n",
    "        x_train.append(data)\n",
    "        y_train.append(0.0)  \n",
    "        \n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "# ===================== # the GRU model\n",
    "\n",
    "# size parameters\n",
    "max_frames=50\n",
    "max_objects=30\n",
    "no_of_features=8\n",
    "\n",
    "# Define model parameters\n",
    "num_gru_units = 128\n",
    "num_dense_units = 64\n",
    "dropout_rate = 0.3\n",
    "\n",
    "# load the data for training\n",
    "x_train , y_train =  load_data(f'{HOME}\\\\CCD_Accident_Sequences\\\\train',f'{HOME}\\\\CCD_Normal_Sequences\\\\train')     # read sequences from DESK\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_train = x_train.reshape(3200, -1, max_objects*no_of_features) # flatten each frame to 1D\n",
    "\n",
    "# validation data\n",
    "x_val ,y_val =  load_data( f'{HOME}\\\\CCD_Accident_Sequences\\\\validate',f'{HOME}\\\\CCD_Normal_Sequences\\\\validate'  )     # read sequences from DESK\n",
    "x_val = np.array(x_val)\n",
    "y_val = np.array(y_val)\n",
    "# write the number of validation examples available\n",
    "x_val = x_val.reshape(800, -1, max_objects*no_of_features) # flatten each frame to 1D\n",
    "\n",
    "# Define the model\n",
    "model1 = tf.keras.Sequential()\n",
    "\n",
    "model1.add(Masking( mask_value=0 , input_shape=( max_frames, max_objects*no_of_features)))\n",
    "\n",
    "model1.add(GRU(num_gru_units))\n",
    "model1.add(Dropout(dropout_rate))\n",
    "\n",
    "# Dense layer for prediction\n",
    "model1.add(Dense(num_dense_units, activation='relu' ,kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01) ) )\n",
    "model1.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output layer with sigmoid activation for probability\n",
    "model1.add(Dense(1, activation='sigmoid' , kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01) ))\n",
    "    \n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=['accuracy',f1_m,precision_m,recall_m])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "\n",
    "history = model1.fit(x_train, y_train, epochs=16, batch_size=16, validation_data=(x_val, y_val), callbacks = early_stopping )\n",
    "\n",
    "# Save the model after training\n",
    "model1.save('accident_prediction_GRU_model.keras') \n",
    "\n",
    "with open('historyGRU.pickle', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the history\n",
    "with open('historyGRU.pickle', 'rb') as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(0.4, 0.9)\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Access the values of custom metrics\n",
    "recall_values = history['recall_m']\n",
    "precision_values = history['precision_m']\n",
    "f1_values = history['f1_m']\n",
    "\n",
    "# Plotting\n",
    "\n",
    "plt.plot(recall_values, label='Recall')\n",
    "plt.title('Recall Over Epochs')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(10,50)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(precision_values, label='Precision')\n",
    "plt.title('Precision over Epochs')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(10,50)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(f1_values, label='F1 Score')\n",
    "plt.title('F1 Score over Epochs')\n",
    "plt.ylabel('F1_score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylim(10, 50)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
