{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the LSTM model on a saved video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1088x1920 2 car_backs, 745.5ms\n",
      "Speed: 45.9ms preprocess, 745.5ms inference, 6.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5052167\n",
      "\n",
      "0: 1088x1920 2 car_backs, 877.3ms\n",
      "Speed: 69.9ms preprocess, 877.3ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50519335\n",
      "\n",
      "0: 1088x1920 3 car_backs, 788.2ms\n",
      "Speed: 57.3ms preprocess, 788.2ms inference, 4.1ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50975573\n",
      "\n",
      "0: 1088x1920 2 car_backs, 799.6ms\n",
      "Speed: 56.1ms preprocess, 799.6ms inference, 6.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5081383\n",
      "\n",
      "0: 1088x1920 4 car_backs, 777.6ms\n",
      "Speed: 62.1ms preprocess, 777.6ms inference, 5.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5082028\n",
      "\n",
      "0: 1088x1920 3 car_backs, 810.4ms\n",
      "Speed: 65.0ms preprocess, 810.4ms inference, 4.1ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5081676\n",
      "\n",
      "0: 1088x1920 3 car_backs, 796.9ms\n",
      "Speed: 55.4ms preprocess, 796.9ms inference, 5.9ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50527835\n",
      "\n",
      "0: 1088x1920 8 car_backs, 830.7ms\n",
      "Speed: 71.7ms preprocess, 830.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5091764\n",
      "\n",
      "0: 1088x1920 3 car_backs, 799.0ms\n",
      "Speed: 57.4ms preprocess, 799.0ms inference, 7.0ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5088935\n",
      "\n",
      "0: 1088x1920 4 car_backs, 816.1ms\n",
      "Speed: 66.8ms preprocess, 816.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5038246\n",
      "\n",
      "0: 1088x1920 3 car_backs, 816.2ms\n",
      "Speed: 51.6ms preprocess, 816.2ms inference, 4.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50941586\n",
      "\n",
      "0: 1088x1920 2 car_backs, 808.3ms\n",
      "Speed: 52.2ms preprocess, 808.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5094381\n",
      "\n",
      "0: 1088x1920 2 car_backs, 772.4ms\n",
      "Speed: 62.4ms preprocess, 772.4ms inference, 3.2ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5176699\n",
      "\n",
      "0: 1088x1920 3 car_backs, 800.0ms\n",
      "Speed: 52.7ms preprocess, 800.0ms inference, 9.6ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5187147\n",
      "\n",
      "0: 1088x1920 3 car_backs, 797.5ms\n",
      "Speed: 50.6ms preprocess, 797.5ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51911926\n",
      "\n",
      "0: 1088x1920 3 car_backs, 810.9ms\n",
      "Speed: 61.6ms preprocess, 810.9ms inference, 6.2ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51919556\n",
      "\n",
      "0: 1088x1920 2 car_backs, 790.4ms\n",
      "Speed: 55.0ms preprocess, 790.4ms inference, 4.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51924294\n",
      "\n",
      "0: 1088x1920 2 car_backs, 824.0ms\n",
      "Speed: 51.4ms preprocess, 824.0ms inference, 8.0ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5192193\n",
      "\n",
      "0: 1088x1920 3 car_backs, 867.1ms\n",
      "Speed: 55.6ms preprocess, 867.1ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.518845\n",
      "\n",
      "0: 1088x1920 2 car_backs, 778.3ms\n",
      "Speed: 85.2ms preprocess, 778.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51909196\n",
      "\n",
      "0: 1088x1920 1 car_back, 775.1ms\n",
      "Speed: 64.7ms preprocess, 775.1ms inference, 3.5ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5190722\n",
      "\n",
      "0: 1088x1920 2 car_backs, 766.6ms\n",
      "Speed: 55.0ms preprocess, 766.6ms inference, 9.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5190358\n",
      "\n",
      "0: 1088x1920 1 car_back, 766.4ms\n",
      "Speed: 62.4ms preprocess, 766.4ms inference, 6.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5057864\n",
      "\n",
      "0: 1088x1920 1 car_back, 806.0ms\n",
      "Speed: 47.4ms preprocess, 806.0ms inference, 6.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.505147\n",
      "\n",
      "0: 1088x1920 1 car_back, 806.3ms\n",
      "Speed: 53.7ms preprocess, 806.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5031947\n",
      "\n",
      "0: 1088x1920 1 car_back, 823.5ms\n",
      "Speed: 68.7ms preprocess, 823.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50374126\n",
      "\n",
      "0: 1088x1920 1 car_back, 830.7ms\n",
      "Speed: 52.2ms preprocess, 830.7ms inference, 5.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50155556\n",
      "\n",
      "0: 1088x1920 1 car_back, 830.1ms\n",
      "Speed: 54.5ms preprocess, 830.1ms inference, 2.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.49959466\n",
      "\n",
      "0: 1088x1920 1 car_back, 824.9ms\n",
      "Speed: 66.6ms preprocess, 824.9ms inference, 5.9ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5006135\n",
      "\n",
      "0: 1088x1920 1 car_back, 975.6ms\n",
      "Speed: 75.2ms preprocess, 975.6ms inference, 3.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5048847\n",
      "\n",
      "0: 1088x1920 1 car_back, 829.1ms\n",
      "Speed: 57.6ms preprocess, 829.1ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.505852\n",
      "\n",
      "0: 1088x1920 1 car_back, 820.7ms\n",
      "Speed: 61.9ms preprocess, 820.7ms inference, 6.5ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5006802\n",
      "\n",
      "0: 1088x1920 1 car_back, 805.2ms\n",
      "Speed: 90.6ms preprocess, 805.2ms inference, 3.9ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5059339\n",
      "\n",
      "0: 1088x1920 1 car_back, 811.1ms\n",
      "Speed: 64.9ms preprocess, 811.1ms inference, 5.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5039551\n",
      "\n",
      "0: 1088x1920 (no detections), 878.9ms\n",
      "Speed: 64.5ms preprocess, 878.9ms inference, 1.9ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5039551\n",
      "\n",
      "0: 1088x1920 6 car_backs, 862.7ms\n",
      "Speed: 66.3ms preprocess, 862.7ms inference, 6.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51356655\n",
      "\n",
      "0: 1088x1920 12 car_backs, 881.8ms\n",
      "Speed: 59.1ms preprocess, 881.8ms inference, 3.6ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5143839\n",
      "\n",
      "0: 1088x1920 4 car_backs, 851.1ms\n",
      "Speed: 57.8ms preprocess, 851.1ms inference, 7.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5132487\n",
      "\n",
      "0: 1088x1920 2 car_backs, 857.6ms\n",
      "Speed: 55.9ms preprocess, 857.6ms inference, 5.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51114786\n",
      "\n",
      "0: 1088x1920 4 car_backs, 859.4ms\n",
      "Speed: 60.4ms preprocess, 859.4ms inference, 0.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51690817\n",
      "\n",
      "0: 1088x1920 6 car_backs, 830.6ms\n",
      "Speed: 89.4ms preprocess, 830.6ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5168953\n",
      "\n",
      "0: 1088x1920 3 car_backs, 817.0ms\n",
      "Speed: 55.9ms preprocess, 817.0ms inference, 4.2ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5137384\n",
      "\n",
      "0: 1088x1920 5 car_backs, 856.2ms\n",
      "Speed: 71.2ms preprocess, 856.2ms inference, 3.5ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5103514\n",
      "\n",
      "0: 1088x1920 3 car_backs, 862.7ms\n",
      "Speed: 60.8ms preprocess, 862.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51461744\n",
      "\n",
      "0: 1088x1920 4 car_backs, 866.8ms\n",
      "Speed: 50.0ms preprocess, 866.8ms inference, 2.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5145961\n",
      "\n",
      "0: 1088x1920 4 car_backs, 846.6ms\n",
      "Speed: 67.5ms preprocess, 846.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5135685\n",
      "\n",
      "0: 1088x1920 2 car_backs, 803.7ms\n",
      "Speed: 57.0ms preprocess, 803.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5155802\n",
      "\n",
      "0: 1088x1920 4 car_backs, 819.3ms\n",
      "Speed: 56.8ms preprocess, 819.3ms inference, 3.6ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5117424\n",
      "\n",
      "0: 1088x1920 6 car_backs, 850.7ms\n",
      "Speed: 54.2ms preprocess, 850.7ms inference, 6.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5123049\n",
      "\n",
      "0: 1088x1920 4 car_backs, 955.0ms\n",
      "Speed: 53.5ms preprocess, 955.0ms inference, 4.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5103825\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tensorflow import keras\n",
    "from collections import defaultdict, deque\n",
    "import os\n",
    "\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Load the models\n",
    "model_path = os.path.join(HOME,'runs\\\\detect\\\\train\\\\weights\\\\best.pt')\n",
    "YOLO_model = YOLO(model_path) \n",
    "LSTM_model = keras.models.load_model(f\"{HOME}\\\\accident_prediction_LSTM_model.keras\")\n",
    "\n",
    "\n",
    "# Open the video file\n",
    "video_path = 'video.mp4'\n",
    "                   \n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "max_frames=50\n",
    "max_objects=30\n",
    "no_of_features=8\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Initialize a deque to hold the latest 50 processed frames\n",
    "sequences = deque(maxlen=50)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if success:\n",
    "        # Run YOLOv9 inference on the frame\n",
    "        results = YOLO_model.track(frame,persist=True)\n",
    "        current_sequence =[None]*max_objects\n",
    "        flag = False\n",
    "        if results is not None and results[0].boxes is not None:\n",
    "            flag = True\n",
    "            boxes = results[0].boxes.xywh.cpu().tolist()\n",
    "\n",
    "            if results[0].boxes.id is not None :\n",
    "                track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            else :\n",
    "                track_ids = [0]*len(results[0].boxes.xyxy)\n",
    "\n",
    "            confidences = results[0].boxes.conf.tolist()\n",
    "            classes = results[0].boxes.cls.tolist()\n",
    "            for box, track_id, conf, classID in zip(boxes, track_ids, confidences, classes):\n",
    "                # extract the class and the orientation from the class_id\n",
    "                if(int(classID)==1):\n",
    "                    class_id = 1\n",
    "                    orientation = 1\n",
    "                elif(int(classID)==2):\n",
    "                    class_id = 1\n",
    "                    orientation = 2\n",
    "\n",
    "                elif(int(classID)==3):\n",
    "                    class_id = 1\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==4):\n",
    "                    class_id = 2\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==5):\n",
    "                    class_id = 2\n",
    "\n",
    "                elif(int(classID)==6):\n",
    "                    class_id = 2\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==7):\n",
    "                    class_id = 3\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==8):\n",
    "                    class_id = 3\n",
    "                    orientation = 2\n",
    "\n",
    "                elif(int(classID)==9):\n",
    "                    class_id = 3\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==10):\n",
    "                    class_id = 4\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==11):\n",
    "                    class_id = 4\n",
    "                    orientation = 2\n",
    "\n",
    "                elif(int(classID)==12):\n",
    "                    class_id = 4\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==13):\n",
    "                    class_id = 5\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==14):\n",
    "                    class_id = 5\n",
    "                    orientation = 2\n",
    "\n",
    "                else: #int(classID)==15\n",
    "                    class_id = 5\n",
    "                    orientation = 3\n",
    "\n",
    "                x, y, w, h = box   \n",
    "                conf = float (int (conf * 1000) / 1000)\n",
    "                track = track_history[track_id]\n",
    "                distance_moved = 0\n",
    "                if(len(track)!=0):\n",
    "                    prev_position = track[-1]\n",
    "                    distance_moved = math.sqrt( math.pow(float(x)-prev_position[0] , 2) + math.pow(float(y)-prev_position[1] , 2) )\n",
    "                track.append((float(x), float(y)))  # x, y center point  \n",
    "                # add the object info to the list , keep consistency in sequences by TRACK_id\n",
    "                if(track_id<30):\n",
    "                    current_sequence[track_id] = [ x, y, w, h, distance_moved, int(class_id) , int(orientation), conf]\n",
    "\n",
    "            # replace the None values with list of zeros for padding \n",
    "            for i in range(len(current_sequence)):\n",
    "                if(current_sequence[i] is None):\n",
    "                    current_sequence[i] = [0]*no_of_features  \n",
    "\n",
    "            sequences.append(current_sequence)  \n",
    "\n",
    "        # no detected objects in the frame -> put zeros\n",
    "        if not flag:\n",
    "            sequences.append([[0]*no_of_features]*max_objects)\n",
    "        \n",
    "        input_LSTM = list(sequences)\n",
    "        # Pad the video to have a maximum of 50 frames\n",
    "        while len(input_LSTM) < max_frames:\n",
    "            input_LSTM.append([[0]*no_of_features]*max_objects)\n",
    "\n",
    "        input_LSTM = np.array(input_LSTM)\n",
    "        input_LSTM = input_LSTM.reshape(1,-1, max_objects*no_of_features) # flatten each frame to 1D\n",
    "\n",
    "        # run the lstm model\n",
    "        predictions = LSTM_model.predict(input_LSTM)\n",
    "        \n",
    "        predicted_class = np.argmax(predictions, axis=1)  # Get the class (accident/non accident)\n",
    "        confidence_score = predictions[0, predicted_class[0]]  # Access the corresponding probability\n",
    "                    \n",
    "        print(\"Predicted class:\", predicted_class[0])\n",
    "        print(\"Confidence score:\", confidence_score)\n",
    "\n",
    "        # # Visualize the object detection results on the frame\n",
    "        # frame = results[0].plot()\n",
    "\n",
    "        if(predicted_class==1):\n",
    "            #If it's not an accident, calculate the complementary value of the confidence score  \n",
    "            confidence_score =1 - confidence_score \n",
    "\n",
    "        # Draw the rectangle (i.e., the box)\n",
    "        if( int(confidence_score * 100) / 100  > 0.5  ):\n",
    "            cv2.rectangle(frame, (50, 50) , (550, 100) , (0, 0, 255), 1)\n",
    "            cv2.putText(frame, \"accident score \" + str(float (int (confidence_score * 1000) / 1000))+\" WARNING\" , (50, 80) ,cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255) , 2)\n",
    "        else:\n",
    "            cv2.rectangle(frame, (50, 50) , (400, 100) , (255, 0, 0), 1)\n",
    "            cv2.putText(frame, \"accident score \" + str(float (int (confidence_score * 1000) / 1000)) , (50, 80) ,cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) , 2)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"Accident Prediction Inference\",frame)\n",
    "\n",
    "        # Break the loop if ESC is pressed\n",
    "        k = cv2.waitKey(1)\n",
    "        if k%256 == 27:\n",
    "            print(\"Escape hit, closing...\")\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the LSTM model on a camera stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\Nada\\\\yoloproject\\\\accident prediction\\\\Github project\\\\Safe-Intelligent-transportation\\\\runs\\\\detect\\\\train\\\\weights\\\\best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the models\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(HOME,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m YOLO_model \u001b[38;5;241m=\u001b[39m YOLO(model_path) \n\u001b[0;32m     14\u001b[0m LSTM_model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124maccident_prediction_LSTM_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m max_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model\u001b[38;5;241m=\u001b[39mmodel, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:151\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, model, task, verbose)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load(model, task\u001b[38;5;241m=\u001b[39mtask)\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:240\u001b[0m, in \u001b[0;36mModel._load\u001b[1;34m(self, weights, task)\u001b[0m\n\u001b[0;32m    237\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m attempt_load_one_weight(weights)\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:806\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[1;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 806\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m torch_safe_load(weight)  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:732\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[1;34m(weight)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    725\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m temporary_modules(\n\u001b[0;32m    726\u001b[0m         {\n\u001b[0;32m    727\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.yolo.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    730\u001b[0m         }\n\u001b[0;32m    731\u001b[0m     ):  \u001b[38;5;66;03m# for legacy 8.0 Classify and Pose models\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m         ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(file, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:997\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    995\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 997\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    998\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    999\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:444\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\Nada\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:425\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 425\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\Nada\\\\yoloproject\\\\accident prediction\\\\Github project\\\\Safe-Intelligent-transportation\\\\runs\\\\detect\\\\train\\\\weights\\\\best.pt'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tensorflow import keras\n",
    "from collections import defaultdict, deque\n",
    "import os\n",
    "\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Load the models\n",
    "model_path = os.path.join(HOME,'runs\\\\detect\\\\train\\\\weights\\\\best.pt')\n",
    "YOLO_model = YOLO(model_path) \n",
    "LSTM_model = keras.models.load_model(f\"{HOME}\\\\accident_prediction_LSTM_model.keras\")\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "max_frames=50\n",
    "max_objects=30\n",
    "no_of_features=8\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Initialize a deque to hold the latest 50 processed frames\n",
    "sequences = deque(maxlen=50)\n",
    "\n",
    "while(True):\n",
    "    ret, frame = camera.read()\n",
    "    if not ret:\n",
    "        print(\"failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Run YOLOv9 inference on the frame\n",
    "    results = YOLO_model.track(frame,persist=True)\n",
    "    current_sequence =[None]*max_objects\n",
    "    flag = False\n",
    "    if results is not None and results[0].boxes is not None:\n",
    "        flag = True\n",
    "        boxes = results[0].boxes.xywh.cpu().tolist()\n",
    "        # boxes = scaler.fit_transform(boxes)\n",
    "        if results[0].boxes.id is not None :\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        else :\n",
    "            track_ids = [0]*len(results[0].boxes.xyxy)\n",
    "\n",
    "        confidences = results[0].boxes.conf.tolist()\n",
    "        classes = results[0].boxes.cls.tolist()\n",
    "        for box, track_id, conf, classID in zip(boxes, track_ids, confidences, classes):\n",
    "            # extract the class and the orientation from the class_id\n",
    "            if(int(classID)==1):\n",
    "                class_id = 1\n",
    "                orientation = 1\n",
    "            elif(int(classID)==2):\n",
    "                class_id = 1\n",
    "                orientation = 2\n",
    "\n",
    "            elif(int(classID)==3):\n",
    "                class_id = 1\n",
    "                orientation = 3\n",
    "\n",
    "            elif(int(classID)==4):\n",
    "                class_id = 2\n",
    "                orientation = 1\n",
    "\n",
    "            elif(int(classID)==5):\n",
    "                class_id = 2\n",
    "\n",
    "            elif(int(classID)==6):\n",
    "                class_id = 2\n",
    "                orientation = 3\n",
    "\n",
    "            elif(int(classID)==7):\n",
    "                class_id = 3\n",
    "                orientation = 1\n",
    "\n",
    "            elif(int(classID)==8):\n",
    "                class_id = 3\n",
    "                orientation = 2\n",
    "\n",
    "            elif(int(classID)==9):\n",
    "                class_id = 3\n",
    "                orientation = 3\n",
    "\n",
    "            elif(int(classID)==10):\n",
    "                class_id = 4\n",
    "                orientation = 1\n",
    "\n",
    "            elif(int(classID)==11):\n",
    "                class_id = 4\n",
    "                orientation = 2\n",
    "\n",
    "            elif(int(classID)==12):\n",
    "                class_id = 4\n",
    "                orientation = 3\n",
    "\n",
    "            elif(int(classID)==13):\n",
    "                class_id = 5\n",
    "                orientation = 1\n",
    "\n",
    "            elif(int(classID)==14):\n",
    "                class_id = 5\n",
    "                orientation = 2\n",
    "\n",
    "            else: #int(classID)==15\n",
    "                class_id = 5\n",
    "                orientation = 3\n",
    "\n",
    "            x, y, w, h = box   \n",
    "            conf = float (int (conf * 1000) / 1000)\n",
    "            track = track_history[track_id]\n",
    "            distance_moved = 0\n",
    "            if(len(track)!=0):\n",
    "                prev_position = track[-1]\n",
    "                distance_moved = math.sqrt( math.pow(float(x)-prev_position[0] , 2) + math.pow(float(y)-prev_position[1] , 2) )\n",
    "            track.append((float(x), float(y)))  # x, y center point  \n",
    "            # add the object info to the list , keep consistency in sequences by TRACK_id\n",
    "            if(track_id<30):\n",
    "                current_sequence[track_id] = [ x, y, w, h, distance_moved, int(class_id) , int(orientation), conf]\n",
    "\n",
    "        # replace the None values with list of zeros for padding \n",
    "        for i in range(len(current_sequence)):\n",
    "            if(current_sequence[i] is None):\n",
    "                current_sequence[i] = [0]*no_of_features  \n",
    "\n",
    "        sequences.append(current_sequence)  \n",
    "\n",
    "    # no detected objects in the frame -> put zeros\n",
    "    if not flag:\n",
    "        sequences.append([[0]*no_of_features]*max_objects)\n",
    "\n",
    "    input_LSTM = list(sequences)\n",
    "    # Pad the video to have a maximum of 50 frames\n",
    "    while len(input_LSTM) < max_frames:\n",
    "        input_LSTM.append([[0]*no_of_features]*max_objects)\n",
    "\n",
    "    input_LSTM = np.array(input_LSTM)\n",
    "    input_LSTM = input_LSTM.reshape(1,-1, max_objects*no_of_features) # flatten each frame to 1D\n",
    "\n",
    "    # run the lstm model\n",
    "    predictions = LSTM_model.predict(input_LSTM)\n",
    "\n",
    "    predicted_class = np.argmax(predictions, axis=1)  # Get the index of the most likely class\n",
    "    confidence_score = predictions[0, predicted_class[0]]  # Access the corresponding probability\n",
    "                \n",
    "    print(\"Predicted class:\", predicted_class[0])\n",
    "    print(\"Confidence score:\", confidence_score)\n",
    "\n",
    "    # # Visualize the object detection results on the frame\n",
    "    # frame = results[0].plot()\n",
    "\n",
    "    if(predicted_class==1):\n",
    "        #If it's not an accident, calculate the complementary value of the confidence score  \n",
    "        confidence_score =1 - confidence_score\n",
    "\n",
    "    # Draw the rectangle (i.e., the box)\n",
    "    if( int(confidence_score * 100) / 100  > 0.5  ):\n",
    "        cv2.rectangle(frame, (50, 50) , (550, 100) , (0, 0, 255), 1)\n",
    "        cv2.putText(frame, \"accident score \" + str(float (int (confidence_score * 1000) / 1000))+\" WARNING\" , (50, 80) ,cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255) , 2)\n",
    "    else:\n",
    "        cv2.rectangle(frame, (50, 50) , (400, 100) , (255, 0, 0), 1)\n",
    "        cv2.putText(frame, \"accident score \" + str(float (int (confidence_score * 1000) / 1000)) , (50, 80) ,cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) , 2)\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"Accident Prediction Inference\",frame)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "    if k%256 == 27:\n",
    "        # ESC pressed\n",
    "        print(\"Escape hit, closing...\")\n",
    "        break\n",
    "\n",
    "camera.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
