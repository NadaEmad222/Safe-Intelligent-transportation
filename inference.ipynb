{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the LSTM model on a saved video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1088x1920 2 car_backs, 745.5ms\n",
      "Speed: 45.9ms preprocess, 745.5ms inference, 6.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5052167\n",
      "\n",
      "0: 1088x1920 2 car_backs, 877.3ms\n",
      "Speed: 69.9ms preprocess, 877.3ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50519335\n",
      "\n",
      "0: 1088x1920 3 car_backs, 788.2ms\n",
      "Speed: 57.3ms preprocess, 788.2ms inference, 4.1ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50975573\n",
      "\n",
      "0: 1088x1920 2 car_backs, 799.6ms\n",
      "Speed: 56.1ms preprocess, 799.6ms inference, 6.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5081383\n",
      "\n",
      "0: 1088x1920 4 car_backs, 777.6ms\n",
      "Speed: 62.1ms preprocess, 777.6ms inference, 5.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5082028\n",
      "\n",
      "0: 1088x1920 3 car_backs, 810.4ms\n",
      "Speed: 65.0ms preprocess, 810.4ms inference, 4.1ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5081676\n",
      "\n",
      "0: 1088x1920 3 car_backs, 796.9ms\n",
      "Speed: 55.4ms preprocess, 796.9ms inference, 5.9ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50527835\n",
      "\n",
      "0: 1088x1920 8 car_backs, 830.7ms\n",
      "Speed: 71.7ms preprocess, 830.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5091764\n",
      "\n",
      "0: 1088x1920 3 car_backs, 799.0ms\n",
      "Speed: 57.4ms preprocess, 799.0ms inference, 7.0ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5088935\n",
      "\n",
      "0: 1088x1920 4 car_backs, 816.1ms\n",
      "Speed: 66.8ms preprocess, 816.1ms inference, 3.2ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5038246\n",
      "\n",
      "0: 1088x1920 3 car_backs, 816.2ms\n",
      "Speed: 51.6ms preprocess, 816.2ms inference, 4.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50941586\n",
      "\n",
      "0: 1088x1920 2 car_backs, 808.3ms\n",
      "Speed: 52.2ms preprocess, 808.3ms inference, 2.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5094381\n",
      "\n",
      "0: 1088x1920 2 car_backs, 772.4ms\n",
      "Speed: 62.4ms preprocess, 772.4ms inference, 3.2ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5176699\n",
      "\n",
      "0: 1088x1920 3 car_backs, 800.0ms\n",
      "Speed: 52.7ms preprocess, 800.0ms inference, 9.6ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5187147\n",
      "\n",
      "0: 1088x1920 3 car_backs, 797.5ms\n",
      "Speed: 50.6ms preprocess, 797.5ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51911926\n",
      "\n",
      "0: 1088x1920 3 car_backs, 810.9ms\n",
      "Speed: 61.6ms preprocess, 810.9ms inference, 6.2ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51919556\n",
      "\n",
      "0: 1088x1920 2 car_backs, 790.4ms\n",
      "Speed: 55.0ms preprocess, 790.4ms inference, 4.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51924294\n",
      "\n",
      "0: 1088x1920 2 car_backs, 824.0ms\n",
      "Speed: 51.4ms preprocess, 824.0ms inference, 8.0ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5192193\n",
      "\n",
      "0: 1088x1920 3 car_backs, 867.1ms\n",
      "Speed: 55.6ms preprocess, 867.1ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.518845\n",
      "\n",
      "0: 1088x1920 2 car_backs, 778.3ms\n",
      "Speed: 85.2ms preprocess, 778.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51909196\n",
      "\n",
      "0: 1088x1920 1 car_back, 775.1ms\n",
      "Speed: 64.7ms preprocess, 775.1ms inference, 3.5ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5190722\n",
      "\n",
      "0: 1088x1920 2 car_backs, 766.6ms\n",
      "Speed: 55.0ms preprocess, 766.6ms inference, 9.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5190358\n",
      "\n",
      "0: 1088x1920 1 car_back, 766.4ms\n",
      "Speed: 62.4ms preprocess, 766.4ms inference, 6.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5057864\n",
      "\n",
      "0: 1088x1920 1 car_back, 806.0ms\n",
      "Speed: 47.4ms preprocess, 806.0ms inference, 6.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.505147\n",
      "\n",
      "0: 1088x1920 1 car_back, 806.3ms\n",
      "Speed: 53.7ms preprocess, 806.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5031947\n",
      "\n",
      "0: 1088x1920 1 car_back, 823.5ms\n",
      "Speed: 68.7ms preprocess, 823.5ms inference, 3.5ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50374126\n",
      "\n",
      "0: 1088x1920 1 car_back, 830.7ms\n",
      "Speed: 52.2ms preprocess, 830.7ms inference, 5.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.50155556\n",
      "\n",
      "0: 1088x1920 1 car_back, 830.1ms\n",
      "Speed: 54.5ms preprocess, 830.1ms inference, 2.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.49959466\n",
      "\n",
      "0: 1088x1920 1 car_back, 824.9ms\n",
      "Speed: 66.6ms preprocess, 824.9ms inference, 5.9ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5006135\n",
      "\n",
      "0: 1088x1920 1 car_back, 975.6ms\n",
      "Speed: 75.2ms preprocess, 975.6ms inference, 3.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5048847\n",
      "\n",
      "0: 1088x1920 1 car_back, 829.1ms\n",
      "Speed: 57.6ms preprocess, 829.1ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.505852\n",
      "\n",
      "0: 1088x1920 1 car_back, 820.7ms\n",
      "Speed: 61.9ms preprocess, 820.7ms inference, 6.5ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5006802\n",
      "\n",
      "0: 1088x1920 1 car_back, 805.2ms\n",
      "Speed: 90.6ms preprocess, 805.2ms inference, 3.9ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5059339\n",
      "\n",
      "0: 1088x1920 1 car_back, 811.1ms\n",
      "Speed: 64.9ms preprocess, 811.1ms inference, 5.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5039551\n",
      "\n",
      "0: 1088x1920 (no detections), 878.9ms\n",
      "Speed: 64.5ms preprocess, 878.9ms inference, 1.9ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5039551\n",
      "\n",
      "0: 1088x1920 6 car_backs, 862.7ms\n",
      "Speed: 66.3ms preprocess, 862.7ms inference, 6.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51356655\n",
      "\n",
      "0: 1088x1920 12 car_backs, 881.8ms\n",
      "Speed: 59.1ms preprocess, 881.8ms inference, 3.6ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5143839\n",
      "\n",
      "0: 1088x1920 4 car_backs, 851.1ms\n",
      "Speed: 57.8ms preprocess, 851.1ms inference, 7.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5132487\n",
      "\n",
      "0: 1088x1920 2 car_backs, 857.6ms\n",
      "Speed: 55.9ms preprocess, 857.6ms inference, 5.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51114786\n",
      "\n",
      "0: 1088x1920 4 car_backs, 859.4ms\n",
      "Speed: 60.4ms preprocess, 859.4ms inference, 0.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51690817\n",
      "\n",
      "0: 1088x1920 6 car_backs, 830.6ms\n",
      "Speed: 89.4ms preprocess, 830.6ms inference, 4.3ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5168953\n",
      "\n",
      "0: 1088x1920 3 car_backs, 817.0ms\n",
      "Speed: 55.9ms preprocess, 817.0ms inference, 4.2ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5137384\n",
      "\n",
      "0: 1088x1920 5 car_backs, 856.2ms\n",
      "Speed: 71.2ms preprocess, 856.2ms inference, 3.5ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5103514\n",
      "\n",
      "0: 1088x1920 3 car_backs, 862.7ms\n",
      "Speed: 60.8ms preprocess, 862.7ms inference, 2.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.51461744\n",
      "\n",
      "0: 1088x1920 4 car_backs, 866.8ms\n",
      "Speed: 50.0ms preprocess, 866.8ms inference, 2.4ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5145961\n",
      "\n",
      "0: 1088x1920 4 car_backs, 846.6ms\n",
      "Speed: 67.5ms preprocess, 846.6ms inference, 2.8ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5135685\n",
      "\n",
      "0: 1088x1920 2 car_backs, 803.7ms\n",
      "Speed: 57.0ms preprocess, 803.7ms inference, 3.0ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5155802\n",
      "\n",
      "0: 1088x1920 4 car_backs, 819.3ms\n",
      "Speed: 56.8ms preprocess, 819.3ms inference, 3.6ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5117424\n",
      "\n",
      "0: 1088x1920 6 car_backs, 850.7ms\n",
      "Speed: 54.2ms preprocess, 850.7ms inference, 6.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5123049\n",
      "\n",
      "0: 1088x1920 4 car_backs, 955.0ms\n",
      "Speed: 53.5ms preprocess, 955.0ms inference, 4.7ms postprocess per image at shape (1, 3, 1088, 1920)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Predicted class: 0\n",
      "Confidence score: 0.5103825\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tensorflow import keras\n",
    "from collections import defaultdict, deque\n",
    "import os\n",
    "\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Load the models\n",
    "model_path = os.path.join(HOME,'runs\\\\detect\\\\train\\\\weights\\\\best.pt')\n",
    "YOLO_model = YOLO(model_path) \n",
    "LSTM_model = keras.models.load_model(f\"{HOME}\\\\accident_prediction_LSTM_model.keras\")\n",
    "\n",
    "\n",
    "# Open the video file\n",
    "video_path = 'video.mp4'\n",
    "                   \n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "max_frames=50\n",
    "max_objects=30\n",
    "no_of_features=8\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Initialize a deque to hold the latest 50 processed frames\n",
    "sequences = deque(maxlen=50)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = YOLO_model.track(frame,persist=True)\n",
    "        current_sequence =[None]*max_objects\n",
    "        flag = False\n",
    "        if results is not None and results[0].boxes is not None:\n",
    "            flag = True\n",
    "            boxes = results[0].boxes.xywh.cpu().tolist()\n",
    "            # boxes = scaler.fit_transform(boxes)\n",
    "            if results[0].boxes.id is not None :\n",
    "                track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            else :\n",
    "                track_ids = [0]*len(results[0].boxes.xyxy)\n",
    "\n",
    "            confidences = results[0].boxes.conf.tolist()\n",
    "            classes = results[0].boxes.cls.tolist()\n",
    "            for box, track_id, conf, classID in zip(boxes, track_ids, confidences, classes):\n",
    "                # extract the class and the orientation from the class_id\n",
    "                if(int(classID)==1):\n",
    "                    class_id = 1\n",
    "                    orientation = 1\n",
    "                elif(int(classID)==2):\n",
    "                    class_id = 1\n",
    "                    orientation = 2\n",
    "\n",
    "                elif(int(classID)==3):\n",
    "                    class_id = 1\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==4):\n",
    "                    class_id = 2\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==5):\n",
    "                    class_id = 2\n",
    "\n",
    "                elif(int(classID)==6):\n",
    "                    class_id = 2\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==7):\n",
    "                    class_id = 3\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==8):\n",
    "                    class_id = 3\n",
    "                    orientation = 2\n",
    "\n",
    "                elif(int(classID)==9):\n",
    "                    class_id = 3\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==10):\n",
    "                    class_id = 4\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==11):\n",
    "                    class_id = 4\n",
    "                    orientation = 2\n",
    "\n",
    "                elif(int(classID)==12):\n",
    "                    class_id = 4\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==13):\n",
    "                    class_id = 5\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==14):\n",
    "                    class_id = 5\n",
    "                    orientation = 2\n",
    "\n",
    "                else: #int(classID)==15\n",
    "                    class_id = 5\n",
    "                    orientation = 3\n",
    "\n",
    "                x, y, w, h = box   \n",
    "                conf = float (int (conf * 1000) / 1000)\n",
    "                track = track_history[track_id]\n",
    "                distance_moved = 0\n",
    "                if(len(track)!=0):\n",
    "                    prev_position = track[-1]\n",
    "                    distance_moved = math.sqrt( math.pow(float(x)-prev_position[0] , 2) + math.pow(float(y)-prev_position[1] , 2) )\n",
    "                track.append((float(x), float(y)))  # x, y center point  \n",
    "                # add the object info to the list , keep consistency in sequences by TRACK_id\n",
    "                if(track_id<30):\n",
    "                    current_sequence[track_id] = [ x, y, w, h, distance_moved, int(class_id) , int(orientation), conf]\n",
    "\n",
    "            # replace the None values with list of zeros for padding \n",
    "            for i in range(len(current_sequence)):\n",
    "                if(current_sequence[i] is None):\n",
    "                    current_sequence[i] = [0]*no_of_features  \n",
    "\n",
    "            sequences.append(current_sequence)  \n",
    "\n",
    "        # no detected objects in the frame -> put zeros\n",
    "        if not flag:\n",
    "            sequences.append([[0]*no_of_features]*max_objects)\n",
    "        \n",
    "        input_LSTM = list(sequences)\n",
    "        # Pad the video to have a maximum of 50 frames\n",
    "        while len(input_LSTM) < max_frames:\n",
    "            input_LSTM.append([[0]*no_of_features]*max_objects)\n",
    "\n",
    "        input_LSTM = np.array(input_LSTM)\n",
    "        input_LSTM = input_LSTM.reshape(1,-1, max_objects*no_of_features) # flatten each frame to 1D\n",
    "\n",
    "        # run the lstm model\n",
    "        predictions = LSTM_model.predict(input_LSTM)\n",
    "        \n",
    "        predicted_class = np.argmax(predictions, axis=1)  # Get the index of the most likely class\n",
    "        confidence_score = predictions[0, predicted_class[0]]  # Access the corresponding probability\n",
    "                    \n",
    "        print(\"Predicted class:\", predicted_class[0])\n",
    "        print(\"Confidence score:\", confidence_score)\n",
    "\n",
    "        # # Visualize the object detection results on the frame\n",
    "        # frame = results[0].plot()\n",
    "\n",
    "        # Draw the rectangle (i.e., the box)\n",
    "        if( int(confidence_score * 100) / 100  > 0.5  ):\n",
    "            cv2.rectangle(frame, (50, 50) , (550, 100) , (0, 0, 255), 1)\n",
    "            cv2.putText(frame, \"accident score \" + str(float (int (confidence_score * 1000) / 1000))+\" WARNING\" , (50, 80) ,cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255) , 2)\n",
    "        else:\n",
    "            cv2.rectangle(frame, (50, 50) , (400, 100) , (255, 0, 0), 1)\n",
    "            cv2.putText(frame, \"accident score \" + str(float (int (confidence_score * 1000) / 1000)) , (50, 80) ,cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) , 2)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"Accident Prediction Inference\",frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the LSTM model on a camera stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTES\n",
    "# (samples, timesteps, features)\n",
    "# samples = size of batch , the batch is the group of examples taken together for a training iteration , so if dataset = 1000 video and the batch = 64 then the iterations will be 1000/64\n",
    "# timesteps = the size of a single sequence (number of frames per sequence )\n",
    "# features = size of the frame list expected to be  1D list of values\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tensorflow import keras\n",
    "from collections import defaultdict, deque\n",
    "import os\n",
    "\n",
    "HOME = os.getcwd()\n",
    "\n",
    "# Load the models\n",
    "model_path = os.path.join(HOME,'runs\\\\detect\\\\train\\\\weights\\\\best.pt')\n",
    "YOLO_model = YOLO(model_path) \n",
    "LSTM_model = keras.models.load_model(f\"{HOME}\\\\accident_prediction_LSTM_model.keras\")\n",
    "\n",
    "max_frames=50\n",
    "max_objects=30\n",
    "no_of_features=8\n",
    "\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Initialize a deque to hold the latest 50 processed frames\n",
    "sequences = deque(maxlen=50)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = YOLO_model.track(frame,persist=True)\n",
    "        current_sequence =[None]*max_objects\n",
    "        flag = False\n",
    "        if results is not None and results[0].boxes is not None:\n",
    "            flag = True\n",
    "            boxes = results[0].boxes.xywh.cpu().tolist()\n",
    "            # boxes = scaler.fit_transform(boxes)\n",
    "            if results[0].boxes.id is not None :\n",
    "                track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            else :\n",
    "                track_ids = [0]*len(results[0].boxes.xyxy)\n",
    "\n",
    "            confidences = results[0].boxes.conf.tolist()\n",
    "            classes = results[0].boxes.cls.tolist()\n",
    "            for box, track_id, conf, classID in zip(boxes, track_ids, confidences, classes):\n",
    "                # extract the class and the orientation from the class_id\n",
    "                if(int(classID)==1):\n",
    "                    class_id = 1\n",
    "                    orientation = 1\n",
    "                elif(int(classID)==2):\n",
    "                    class_id = 1\n",
    "                    orientation = 2\n",
    "\n",
    "                elif(int(classID)==3):\n",
    "                    class_id = 1\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==4):\n",
    "                    class_id = 2\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==5):\n",
    "                    class_id = 2\n",
    "\n",
    "                elif(int(classID)==6):\n",
    "                    class_id = 2\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==7):\n",
    "                    class_id = 3\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==8):\n",
    "                    class_id = 3\n",
    "                    orientation = 2\n",
    "\n",
    "                elif(int(classID)==9):\n",
    "                    class_id = 3\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==10):\n",
    "                    class_id = 4\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==11):\n",
    "                    class_id = 4\n",
    "                    orientation = 2\n",
    "\n",
    "                elif(int(classID)==12):\n",
    "                    class_id = 4\n",
    "                    orientation = 3\n",
    "\n",
    "                elif(int(classID)==13):\n",
    "                    class_id = 5\n",
    "                    orientation = 1\n",
    "\n",
    "                elif(int(classID)==14):\n",
    "                    class_id = 5\n",
    "                    orientation = 2\n",
    "\n",
    "                else: #int(classID)==15\n",
    "                    class_id = 5\n",
    "                    orientation = 3\n",
    "\n",
    "                x, y, w, h = box   \n",
    "                conf = float (int (conf * 1000) / 1000)\n",
    "                track = track_history[track_id]\n",
    "                distance_moved = 0\n",
    "                if(len(track)!=0):\n",
    "                    prev_position = track[-1]\n",
    "                    distance_moved = math.sqrt( math.pow(float(x)-prev_position[0] , 2) + math.pow(float(y)-prev_position[1] , 2) )\n",
    "                track.append((float(x), float(y)))  # x, y center point  \n",
    "                # add the object info to the list , keep consistency in sequences by TRACK_id\n",
    "                if(track_id<30):\n",
    "                    current_sequence[track_id] = [ x, y, w, h, distance_moved, int(class_id) , int(orientation), conf]\n",
    "\n",
    "            # replace the None values with list of zeros for padding \n",
    "            for i in range(len(current_sequence)):\n",
    "                if(current_sequence[i] is None):\n",
    "                    current_sequence[i] = [0]*no_of_features  \n",
    "\n",
    "            sequences.append(current_sequence)  \n",
    "\n",
    "        # no detected objects in the frame -> put zeros\n",
    "        if not flag:\n",
    "            sequences.append([[0]*no_of_features]*max_objects)\n",
    "        \n",
    "        input_LSTM = list(sequences)\n",
    "        # Pad the video to have a maximum of 50 frames\n",
    "        while len(input_LSTM) < max_frames:\n",
    "            input_LSTM.append([[0]*no_of_features]*max_objects)\n",
    "\n",
    "        input_LSTM = np.array(input_LSTM)\n",
    "        input_LSTM = input_LSTM.reshape(1,-1, max_objects*no_of_features) # flatten each frame to 1D\n",
    "\n",
    "        # run the lstm model\n",
    "        predictions = LSTM_model.predict(input_LSTM)\n",
    "        \n",
    "        predicted_class = np.argmax(predictions, axis=1)  # Get the index of the most likely class\n",
    "        confidence_score = predictions[0, predicted_class[0]]  # Access the corresponding probability\n",
    "                    \n",
    "        print(\"Predicted class:\", predicted_class[0])\n",
    "        print(\"Confidence score:\", confidence_score)\n",
    "\n",
    "        # # Visualize the object detection results on the frame\n",
    "        # frame = results[0].plot()\n",
    "\n",
    "        # Draw the rectangle (i.e., the box)\n",
    "        if( int(confidence_score * 100) / 100  > 0.5  ):\n",
    "            cv2.rectangle(frame, (50, 50) , (550, 100) , (0, 0, 255), 1)\n",
    "            cv2.putText(frame, \"accident score \" + str(float (int (confidence_score * 1000) / 1000))+\" WARNING\" , (50, 80) ,cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255) , 2)\n",
    "        else:\n",
    "            cv2.rectangle(frame, (50, 50) , (400, 100) , (255, 0, 0), 1)\n",
    "            cv2.putText(frame, \"accident score \" + str(float (int (confidence_score * 1000) / 1000)) , (50, 80) ,cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) , 2)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"Accident Prediction Inference\",frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
